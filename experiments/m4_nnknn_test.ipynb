{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8fdf85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "module_path = Path.cwd().resolve() / \"..\" /\"src\" / \"nnknn\" / \"nnknn.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"nnknn\", str(module_path))\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod)\n",
    "NNKNN = mod.NNKNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "355b6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_train = pd.read_csv('../src/data/m4_forecasting/Daily-train.csv')\n",
    "daily_test = pd.read_csv('../src/data/m4_forecasting/Daily-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6a615780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(actual, predicted):\n",
    "    # actual = np.asarray(actual).reshape(-1)\n",
    "    # predicted = np.asarray(predicted).reshape(-1)\n",
    "\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "be50ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_daily_series(row):\n",
    "    # Drop the ID in column V1\n",
    "    ts = row.iloc[1:]\n",
    "\n",
    "    # Drop trailing NaNs (uneven lengths)\n",
    "    ts = ts.dropna().astype(float)\n",
    "\n",
    "    # Assign daily index (fake but consistent)\n",
    "    ts.index = pd.date_range(start=\"2000-01-01\", periods=len(ts), freq=\"D\")\n",
    "\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "954baf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_windows(ts, L, H):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(ts) - L - H + 1):\n",
    "        X.append(ts[i:i+L])\n",
    "        Y.append(ts[i+L:i+L+H])\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f46dcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nnknn(X_cases, Y_targets, L, epochs=100, lr=0.01, shared_weights=False):\n",
    "    model = NNKNN(num_features=L, num_cases=X_cases.shape[0], shared_weights=shared_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for _epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # Use the cases themselves as queries for training\n",
    "        y_hat, _, _ = model(X_cases, X_cases, Y_targets)\n",
    "        loss = criterion(y_hat, Y_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def forecast_nnknn(train_row, L=30, H=7, epochs=100, shared_weights=False):\n",
    "    X_cases, Y_targets = build_windows(train_row, L=L, H=H)\n",
    "\n",
    "    # train model\n",
    "    model = train_nnknn(X_cases, Y_targets, L=L, epochs=epochs, shared_weights=shared_weights)\n",
    "\n",
    "    # last window as query\n",
    "    query = torch.tensor(train_row[-L:], dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_hat, _, _ = model(query, X_cases, Y_targets)\n",
    "    \n",
    "    return y_hat.squeeze(0).cpu().numpy()  # shape [H]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f1fccda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nnknn(train_row, test_row, L=30, H=7):\n",
    "\n",
    "    train_ts = clean_daily_series(train_row)\n",
    "    test_ts = clean_daily_series(test_row)\n",
    "    \n",
    "\n",
    "    # Enforce M4 horizon limit (daily test series have length of 14)\n",
    "    H_eval = min(H, len(test_ts))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_ts_scaled = scaler.fit_transform(\n",
    "        train_ts.to_numpy().reshape(-1, 1)\n",
    "    ).flatten()\n",
    "\n",
    "    y_pred = forecast_nnknn(train_ts_scaled, L=L, H=H_eval)\n",
    "    y_pred = np.atleast_1d(y_pred)\n",
    "\n",
    "    y_pred = scaler.inverse_transform(\n",
    "        y_pred.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    y_true = np.atleast_1d(test_ts)\n",
    "\n",
    "    return rmse(y_true[:H], y_pred[:H])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1228b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "python(2809) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(2810) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(2811) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(2812) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.1s\n",
      "/var/folders/6l/zzdt1df940qbq_cc973ylw7m0000gq/T/ipykernel_20237/4030560824.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "/var/folders/6l/zzdt1df940qbq_cc973ylw7m0000gq/T/ipykernel_20237/4030560824.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "/var/folders/6l/zzdt1df940qbq_cc973ylw7m0000gq/T/ipykernel_20237/4030560824.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "/var/folders/6l/zzdt1df940qbq_cc973ylw7m0000gq/T/ipykernel_20237/4030560824.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  7.9min\n",
      "python(4569) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(4596) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/var/folders/6l/zzdt1df940qbq_cc973ylw7m0000gq/T/ipykernel_20237/4030560824.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "python(4631) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/var/folders/6l/zzdt1df940qbq_cc973ylw7m0000gq/T/ipykernel_20237/4030560824.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "/var/folders/6l/zzdt1df940qbq_cc973ylw7m0000gq/T/ipykernel_20237/4030560824.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed: 11.1min remaining:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed: 11.4min remaining:   43.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each series: [179.24913367699904, 25.737294936522726, 43.08965779396816, 338.36090011877695, 243.31954989882482, 677.4823387413791, 3299.9217942391574, 206.99113955110872, 176.24047598142118, 226.39961618328667, 361.9621793896055, 150.60506899444587, 862.64424959999, 1030.7051717659206, 3637.330806320032, 540.1971446847008, 227.59266164555692, 161.0396977191821, 611.9564082030961, 59.386376813806045, 354.12085844841204, 272.86921265292915, 23.124320303289917, 241.19963591163292, 20.533938496610425, 288.35957580844035, 38.730946847217126, 249.70870526800263, 1590.9459039835447, 266.6457338883834, 144.03429193946755, 184.10436833043667, 114.06477154669855, 562.1095184420647, 259.43247983360243, 9.254382384109153, 68.54135123964458, 207.34870670641, 200.10850619918415, 118.47793576494553, 158.86493933934807, 79.72646321539472, 11.920604528188703, 51.10587281165379, 12.534860192598961, 115.75153780830493, 49.29723682611074, 265.9350296781802, 407.07902176208916, 413.64068054116177]\n",
      "mean rmse: 396.7956611391167\n",
      "median rmse: 207.16992312875936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 11.5min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L = 30 #size of window\n",
    "HORIZON = 7 # forecast horizon\n",
    "NUM_SERIES = 50\n",
    "\n",
    "daily_train_copy = daily_train.copy()\n",
    "daily_test_copy = daily_test.copy()\n",
    "\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
    "        delayed(evaluate_nnknn)(daily_train_copy.iloc[i], daily_test_copy.iloc[i], L=L, H=HORIZON)\n",
    "        for i in range(NUM_SERIES)\n",
    "    )\n",
    "print(\"RMSE for each series:\", results)\n",
    "print(f\"mean rmse: {np.mean(results)}\")\n",
    "print(f\"median rmse: {np.median(results)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
